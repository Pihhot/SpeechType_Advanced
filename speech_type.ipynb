{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pihhot/SpeechType_Advanced/blob/main/speech_type.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inputs for NN"
      ],
      "metadata": {
        "id": "Rgia51eqRHVN"
      },
      "id": "Rgia51eqRHVN"
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.utils import to_categorical, pad_sequences\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "from keras.layers import Dense, LSTM, Input, Dropout, Embedding, Concatenate, Input, GRU, Bidirectional\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "pEWPIMVtRKcc"
      },
      "id": "pEWPIMVtRKcc",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Model"
      ],
      "metadata": {
        "id": "sMFOKsarXBX-"
      },
      "id": "sMFOKsarXBX-"
    },
    {
      "cell_type": "code",
      "source": [
        "input_1 = Input(shape=(maxlen,), name='sentence')\n",
        "embedding_1 = Embedding(maxWordsCount, 100, input_length=maxlen)(input_1)\n",
        "\n",
        "\n",
        "input_2 = Input(shape=(maxlen,), name='deps')\n",
        "embedding_2 = Embedding(maxWordsCount, 100, input_length=maxlen)(input_2)\n",
        "\n",
        "\n",
        "input_3 = Input(shape=(maxlen,), name='poss')\n",
        "embedding_3 = Embedding(maxWordsCount, 100, input_length=maxlen)(input_3)\n",
        "\n",
        "\n",
        "concat = Concatenate()([embedding_1, embedding_2, embedding_3])\n",
        "\n",
        "lstm_1 = LSTM(128, dropout=0.1, recurrent_dropout=0.1, return_sequences=True)(concat)\n",
        "\n",
        "lstm_2 = LSTM(64, dropout=0.1, recurrent_dropout=0.1, return_sequences=False)(lstm_1)\n",
        "\n",
        "dense = Dense(43, activation='softmax')(lstm_2)\n",
        "\n",
        "model = Model(inputs=[input_1, input_2, input_3], outputs=dense)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam(0.001))"
      ],
      "metadata": {
        "id": "A-aCquKAXDDm"
      },
      "id": "A-aCquKAXDDm",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit model"
      ],
      "metadata": {
        "id": "-WhhjIYyXbbE"
      },
      "id": "-WhhjIYyXbbE"
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit([sentences_m, deps, poss], classes, batch_size=32, epochs=50, validation_data=([sentences_m_t, deps_t, poss_t], classes_t), shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6qBT_QcXdJk",
        "outputId": "c03b9636-ab6d-44bb-bf60-ca3b4c37cfd8"
      },
      "id": "r6qBT_QcXdJk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "625/625 [==============================] - 44s 63ms/step - loss: 1.7511 - accuracy: 0.5134 - val_loss: 1.5679 - val_accuracy: 0.5540\n",
            "Epoch 2/50\n",
            "625/625 [==============================] - 38s 61ms/step - loss: 1.4919 - accuracy: 0.5600 - val_loss: 1.3799 - val_accuracy: 0.5870\n",
            "Epoch 3/50\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 1.3558 - accuracy: 0.5895 - val_loss: 1.2543 - val_accuracy: 0.6187\n",
            "Epoch 4/50\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 1.2620 - accuracy: 0.6143 - val_loss: 1.1764 - val_accuracy: 0.6351\n",
            "Epoch 5/50\n",
            "625/625 [==============================] - 38s 61ms/step - loss: 1.1981 - accuracy: 0.6318 - val_loss: 1.1159 - val_accuracy: 0.6631\n",
            "Epoch 6/50\n",
            "625/625 [==============================] - 38s 61ms/step - loss: 1.1398 - accuracy: 0.6480 - val_loss: 1.0525 - val_accuracy: 0.6813\n",
            "Epoch 7/50\n",
            "625/625 [==============================] - 37s 59ms/step - loss: 1.0857 - accuracy: 0.6636 - val_loss: 0.9928 - val_accuracy: 0.7012\n",
            "Epoch 8/50\n",
            "625/625 [==============================] - 39s 62ms/step - loss: 1.0265 - accuracy: 0.6822 - val_loss: 0.9339 - val_accuracy: 0.7223\n",
            "Epoch 9/50\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.9676 - accuracy: 0.7009 - val_loss: 0.8567 - val_accuracy: 0.7422\n",
            "Epoch 10/50\n",
            "625/625 [==============================] - 39s 63ms/step - loss: 0.8995 - accuracy: 0.7228 - val_loss: 0.7855 - val_accuracy: 0.7655\n",
            "Epoch 11/50\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.8351 - accuracy: 0.7435 - val_loss: 0.7099 - val_accuracy: 0.7892\n",
            "Epoch 12/50\n",
            "625/625 [==============================] - 38s 61ms/step - loss: 0.7674 - accuracy: 0.7655 - val_loss: 0.6341 - val_accuracy: 0.8090\n",
            "Epoch 13/50\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.6959 - accuracy: 0.7865 - val_loss: 0.5660 - val_accuracy: 0.8345\n",
            "Epoch 14/50\n",
            "625/625 [==============================] - 47s 76ms/step - loss: 0.6339 - accuracy: 0.8063 - val_loss: 0.4992 - val_accuracy: 0.8568\n",
            "Epoch 15/50\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.5697 - accuracy: 0.8280 - val_loss: 0.4526 - val_accuracy: 0.8713\n",
            "Epoch 16/50\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.5160 - accuracy: 0.8448 - val_loss: 0.3927 - val_accuracy: 0.8897\n",
            "Epoch 17/50\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.4683 - accuracy: 0.8594 - val_loss: 0.3609 - val_accuracy: 0.9003\n",
            "Epoch 18/50\n",
            "625/625 [==============================] - 38s 61ms/step - loss: 0.4249 - accuracy: 0.8722 - val_loss: 0.3156 - val_accuracy: 0.9144\n",
            "Epoch 19/50\n",
            "625/625 [==============================] - 39s 63ms/step - loss: 0.3905 - accuracy: 0.8816 - val_loss: 0.2851 - val_accuracy: 0.9203\n",
            "Epoch 20/50\n",
            "625/625 [==============================] - 39s 62ms/step - loss: 0.3613 - accuracy: 0.8906 - val_loss: 0.2634 - val_accuracy: 0.9267\n",
            "Epoch 21/50\n",
            "625/625 [==============================] - 38s 61ms/step - loss: 0.3265 - accuracy: 0.9028 - val_loss: 0.2339 - val_accuracy: 0.9344\n",
            "Epoch 22/50\n",
            "625/625 [==============================] - 39s 62ms/step - loss: 0.3087 - accuracy: 0.9048 - val_loss: 0.2182 - val_accuracy: 0.9412\n",
            "Epoch 23/50\n",
            "625/625 [==============================] - 41s 65ms/step - loss: 0.2833 - accuracy: 0.9143 - val_loss: 0.1985 - val_accuracy: 0.9455\n",
            "Epoch 24/50\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.2703 - accuracy: 0.9177 - val_loss: 0.1862 - val_accuracy: 0.9490\n",
            "Epoch 25/50\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.2553 - accuracy: 0.9215 - val_loss: 0.1735 - val_accuracy: 0.9522\n",
            "Epoch 26/50\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.2366 - accuracy: 0.9292 - val_loss: 0.1515 - val_accuracy: 0.9581\n",
            "Epoch 27/50\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.2193 - accuracy: 0.9319 - val_loss: 0.1421 - val_accuracy: 0.9615\n",
            "Epoch 28/50\n",
            "625/625 [==============================] - 40s 63ms/step - loss: 0.2174 - accuracy: 0.9313 - val_loss: 0.1460 - val_accuracy: 0.9589\n",
            "Epoch 29/50\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.2048 - accuracy: 0.9354 - val_loss: 0.1248 - val_accuracy: 0.9644\n",
            "Epoch 30/50\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.1926 - accuracy: 0.9402 - val_loss: 0.1213 - val_accuracy: 0.9658\n",
            "Epoch 31/50\n",
            "625/625 [==============================] - 36s 58ms/step - loss: 0.1801 - accuracy: 0.9450 - val_loss: 0.1147 - val_accuracy: 0.9664\n",
            "Epoch 32/50\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.1758 - accuracy: 0.9451 - val_loss: 0.1061 - val_accuracy: 0.9689\n",
            "Epoch 33/50\n",
            "625/625 [==============================] - 39s 62ms/step - loss: 0.1730 - accuracy: 0.9456 - val_loss: 0.1038 - val_accuracy: 0.9688\n",
            "Epoch 34/50\n",
            "373/625 [================>.............] - ETA: 12s - loss: 0.1486 - accuracy: 0.9535"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dowload and prepare predprocessed data for NN"
      ],
      "metadata": {
        "id": "SavTb38tA-FO"
      },
      "id": "SavTb38tA-FO"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Dowload data from git\n",
        "#!wget https://raw.githubusercontent.com/Pihhot/SpeechType_Advanced/main/train_data\n",
        "#!wget https://raw.githubusercontent.com/Pihhot/SpeechType_Advanced/main/test_data\n",
        "\n",
        "train_df_path = Path.cwd() / 'train_data'\n",
        "train_df = pd.read_csv(str(train_df_path), sep=':')\n",
        "\n",
        "test_df_path = Path.cwd() / 'train_data'\n",
        "test_df = pd.read_csv(str(train_df_path), sep=':')\n",
        "\n",
        "sentences_m = list(train_df['MODIFIED'])\n",
        "deps = list(train_df['DEPS'])\n",
        "poss = list(train_df['POSS'])\n",
        "classes = list(train_df['TYPE'])\n",
        "\n",
        "sentences_m_t = list(test_df['MODIFIED'])\n",
        "deps_t = list(test_df['DEPS'])\n",
        "poss_t = list(test_df['POSS'])\n",
        "classes_t = list(test_df['TYPE'])\n",
        "\n",
        "\n",
        "# Train Tokenizer\n",
        "maxWordsCount = 4000\n",
        "tokenizer = Tokenizer(num_words=maxWordsCount, filters='!–\"—#$%&amp;()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r«»', lower=True, split=' ', char_level=False)\n",
        "tokenizer.fit_on_texts(sentences_m+deps+poss+sentences_m_t)\n",
        "# print(list(tokenizer.word_counts.items())[:10])\n",
        "\n",
        "# Prepare text information\n",
        "def texts_to_s(texts: list):\n",
        "  return tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "sentences_m, deps, poss = texts_to_s(sentences_m), texts_to_s(deps), texts_to_s(poss)\n",
        "sentences_m_t, deps_t, poss_t = texts_to_s(sentences_m_t), texts_to_s(deps_t), texts_to_s(poss_t)\n",
        "\n",
        "maxlen = 10\n",
        "sentences_m, deps, poss = pad_sequences(sentences_m, maxlen=maxlen), pad_sequences(deps, maxlen=maxlen), pad_sequences(poss, maxlen=maxlen)\n",
        "sentences_m_t, deps_t, poss_t = pad_sequences(sentences_m_t, maxlen=maxlen), pad_sequences(deps_t, maxlen=maxlen), pad_sequences(poss_t, maxlen=maxlen)\n",
        "\n",
        "# Prepare classes\n",
        "classes = np.array(classes)\n",
        "classes_t = np.array(classes)\n",
        "\n",
        "l_encoder = LabelEncoder()\n",
        "\n",
        "classes = l_encoder.fit_transform(classes)\n",
        "classes = to_categorical(classes)\n",
        "\n",
        "classes_t = l_encoder.fit_transform(classes_t)\n",
        "classes_t = to_categorical(classes_t)\n"
      ],
      "metadata": {
        "id": "Avf1ufH2BKnr"
      },
      "id": "Avf1ufH2BKnr",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Download RAW data from git\n",
        "*   Process RAW data\n",
        "*   Split on train & test\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ytpJD30IB3q4"
      },
      "id": "ytpJD30IB3q4"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "!pip install contractions\n",
        "import contractions\n",
        "from pathlib import Path\n",
        "import json\n",
        "import re\n",
        "#####################################################################################\n",
        "!wget https://raw.githubusercontent.com/Pihhot/SpeechType_Advanced/main/data.json\n",
        "\n",
        "data_path = Path.cwd() / 'data.json'\n",
        "\n",
        "with open(data_path) as f:\n",
        "    data = json.load(f)\n",
        "######################################################################################    \n",
        "\n",
        "def sent_lemmas(doc) -> str:\n",
        "    return ' '.join(t.lemma_ for t in doc)\n",
        "\n",
        "def sent_deps(doc) -> str:\n",
        "    return ' '.join(t.dep_ for t in doc)\n",
        "\n",
        "def sent_poss(doc) -> str:\n",
        "    return ' '.join(t.pos_ for t in doc)\n",
        "\n",
        "def clear_sentence(text: str) -> str:\n",
        "    # Remove punctuations and numbers\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    # Single character removal\n",
        "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
        "    # Removing multiple spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.lower()\n",
        "\n",
        "def expand_contractions(text: str) -> str:\n",
        "    return ' '.join([contractions.fix(w) for w in text.split(' ')])\n",
        "\n",
        "data_c = data.copy()\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "train_sentences_or, train_sentences_m, train_deps, train_pos, train_class = [], [], [], [], []\n",
        "test_sentences_or, test_sentences_m, test_deps, test_pos, test_class = [], [], [], [], []\n",
        "\n",
        "\n",
        "index = 0\n",
        "for class_ in data.keys():\n",
        "  for sentence in data[class_]:\n",
        "    index += 1\n",
        "    # Save original sentences\n",
        "    sentence_or = sentence\n",
        "    # Beatify sentence\n",
        "    sentence = expand_contractions(sentence)\n",
        "    sentence = clear_sentence(sentence)\n",
        "    if len(sentence):\n",
        "      # Sentence representations\n",
        "      doc = nlp(sentence)\n",
        "      deps_str = sent_deps(doc)\n",
        "      poss_str = sent_poss(doc)\n",
        "      lemmas_str = sent_lemmas(doc)\n",
        "      # Add every fifth element to test\n",
        "      if index % 7 == 0:\n",
        "        train_sentences_or.append(sentence_or)\n",
        "        train_sentences_m.append(lemmas_str)\n",
        "        train_deps.append(deps_str)\n",
        "        train_pos.append(poss_str)\n",
        "        train_class.append(class_)\n",
        "      else:\n",
        "        test_sentences_or.append(sentence_or)\n",
        "        test_sentences_m.append(lemmas_str)\n",
        "        test_deps.append(deps_str)\n",
        "        test_pos.append(poss_str)\n",
        "        test_class.append(class_)\n",
        "\n",
        "\n",
        "\n",
        "train_df_data = {'ORIGIN':train_sentences_or,\n",
        "                 'MODIFIED':train_sentences_m,\n",
        "                 'DEPS':train_deps,\n",
        "                 'POSS':train_pos,\n",
        "                 'TYPE':train_class}\n",
        "\n",
        "test_df_data = {'ORIGIN':test_sentences_or,\n",
        "                 'MODIFIED':test_sentences_m,\n",
        "                 'DEPS':test_deps,\n",
        "                 'POSS':test_pos,\n",
        "                 'TYPE':test_class}\n",
        "\n",
        "train_df = pd.DataFrame(train_df_data)\n",
        "test_df = pd.DataFrame(test_df_data)\n",
        "\n",
        "test_df.to_csv('test_data.csv', index=False, sep=':')\n",
        "train_df.to_csv('train_data.csv', index=False, sep=':')"
      ],
      "metadata": {
        "id": "_XkuQeJw5EC0"
      },
      "id": "_XkuQeJw5EC0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}